{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2_cloud_data_wh_redshift_boto3\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Amazon_Web_Services_Logo.svg/2560px-Amazon_Web_Services_Logo.svg.png\" width=\"100\" height=\"100\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import configparser\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#!pip install boto3\n",
    "import boto3\n",
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup\n",
    "- Create a **new IAM user**.\n",
    "- Under **Attach existing policies directly**, assign **`AdministratorAccess`**.\n",
    "- Store the **access key** and **secret** in  **2_cloud_data_wh_redshift_boto3.cfg** file (same folder as this notebook).\n",
    "- Fill in the configuration file as follows:\n",
    "    ```\n",
    "    [AWS]\n",
    "    KEY=YOUR_AWS_KEY\n",
    "    SECRET=YOUR_AWS_SECRET\n",
    "    ```\n",
    "\n",
    "## 1.1. Troubleshoot  \n",
    "If your **keys are not working**, such as encountering an **InvalidAccessKeyId** error, create another IAM user with Admin access, or follow these steps to **create a new pair of access keys**.  \n",
    "1. Go to the **[IAM Dashboard](https://console.aws.amazon.com/iam/home)**.  \n",
    "2. View the details of the **Admin user** you created.  \n",
    "3. Select **Security Credentials** â†’ **Create access key**.  \n",
    "4. A new **Access Key ID** and **Secret** will be **generated**.  \n",
    "5. Update the `.cfg` file with the **new** credentials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Load configuration from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration file\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open(\"dwh.cfg\"))\n",
    "\n",
    "# AWS Credentials\n",
    "aws_key = config.get(\"AWS\", \"KEY\")\n",
    "aws_secret = config.get(\"AWS\", \"SECRET\")\n",
    "\n",
    "# AWS Region\n",
    "dwh_region = config.get(\"DWH\", \"DWH_REGION\")\n",
    "\n",
    "# IAM Role\n",
    "dwh_iam_role_name = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")\n",
    "\n",
    "# Redshift Cluster Configuration\n",
    "dwh_cluster_type = config.get(\"DWH\", \"DWH_CLUSTER_TYPE\")\n",
    "dwh_node_type = config.get(\"DWH\", \"DWH_NODE_TYPE\")\n",
    "dwh_num_nodes = config.get(\"DWH\", \"DWH_NUM_NODES\")\n",
    "dwh_cluster_identifier = config.get(\"DWH\", \"DWH_CLUSTER_IDENTIFIER\")\n",
    "\n",
    "# Database Configuration\n",
    "dwh_db = config.get(\"DWH\", \"DWH_DB\")\n",
    "dwh_db_user = config.get(\"DWH\", \"DWH_DB_USER\")\n",
    "dwh_db_password = config.get(\"DWH\", \"DWH_DB_PASSWORD\")\n",
    "dwh_port = config.get(\"DWH\", \"DWH_PORT\")\n",
    "\n",
    "# Display parameters in a DataFrame\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Param\": [\n",
    "            \"DWH_REGION\",\n",
    "            \"DWH_IAM_ROLE_NAME\",\n",
    "            \"DWH_CLUSTER_TYPE\",\n",
    "            \"DWH_NODE_TYPE\",\n",
    "            \"DWH_NUM_NODES\",\n",
    "            \"DWH_CLUSTER_IDENTIFIER\",\n",
    "            \"DWH_DB\",\n",
    "            \"DWH_DB_USER\",\n",
    "            \"DWH_DB_PASSWORD\",\n",
    "            \"DWH_PORT\",\n",
    "        ],\n",
    "        \"Value\": [\n",
    "            dwh_region,\n",
    "            dwh_iam_role_name,\n",
    "            dwh_cluster_type,\n",
    "            dwh_node_type,\n",
    "            dwh_num_nodes,\n",
    "            dwh_cluster_identifier,\n",
    "            dwh_db,\n",
    "            dwh_db_user,\n",
    "            dwh_db_password,\n",
    "            dwh_port,\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Initialize AWS services\n",
    "with credentials from the config file. Choose the same **us-west-2** region in the AWS Web Console to see these resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client(\n",
    "    \"iam\",\n",
    "    region_name=dwh_region,\n",
    "    aws_access_key_id=aws_key,\n",
    "    aws_secret_access_key=aws_secret,\n",
    ")\n",
    "\n",
    "ec2 = boto3.resource(\n",
    "    \"ec2\",\n",
    "    region_name=dwh_region,\n",
    "    aws_access_key_id=aws_key,\n",
    "    aws_secret_access_key=aws_secret,\n",
    ")\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    \"s3\",\n",
    "    region_name=dwh_region,\n",
    "    aws_access_key_id=aws_key,\n",
    "    aws_secret_access_key=aws_secret,\n",
    ")\n",
    "\n",
    "redshift = boto3.client(\n",
    "    \"redshift\",\n",
    "    region_name=dwh_region,\n",
    "    aws_access_key_id=aws_key,\n",
    "    aws_secret_access_key=aws_secret,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. IAM Role Setup\n",
    "Create an IAM role that allows Redshift to access S3 bucket (ReadOnly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the role\n",
    "try:\n",
    "    print(\"1.1 Creating a new IAM Role\")\n",
    "    dwh_role = iam.create_role(\n",
    "        Path=\"/\",\n",
    "        RoleName=dwh_iam_role_name,\n",
    "        Description=\"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps({\n",
    "            \"Statement\": [{\n",
    "                \"Action\": \"sts:AssumeRole\",\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\"Service\": \"redshift.amazonaws.com\"}\n",
    "            }],\n",
    "            \"Version\": \"2012-10-17\"\n",
    "        })\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Attach Policy\n",
    "print(\"1.2 Attaching Policy\")\n",
    "\n",
    "iam.attach_role_policy(RoleName=dwh_iam_role_name,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']\n",
    "\n",
    "# Get and print the IAM role ARN\n",
    "print(\"1.3 Get the IAM role ARN\")\n",
    "role_arn = iam.get_role(RoleName=dwh_iam_role_name)[\"Role\"][\"Arn\"]\n",
    "\n",
    "print(role_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. S3: check sample data, verify dataset presence.  \n",
    "The code lists and prints objects in the S3 bucket \"udacity\" that have keys starting with \"tickets\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_db_bucket = s3.Bucket(\"udacity-labs\")\n",
    "\n",
    "for obj in sample_db_bucket.objects.filter(Prefix=\"tickets\"):\n",
    "    print(obj)\n",
    "    \n",
    "# Uncomment the following lines to list all objects in the bucket\n",
    "#for obj in sample_db_bucket.objects.all():\n",
    "#    print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Redshift: create cluster  \n",
    "Creates a Redshift cluster using the IAM role. For complete arguments to **create_cluster**, see [docs](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift.html#Redshift.Client.create_cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = redshift.create_cluster(\n",
    "        # Parameters for hardware\n",
    "        ClusterType=dwh_cluster_type,\n",
    "        NodeType=dwh_node_type,\n",
    "        NumberOfNodes=int(dwh_num_nodes),\n",
    "\n",
    "        # Parameters for identifiers & credentials\n",
    "        DBName=dwh_db,\n",
    "        ClusterIdentifier=dwh_cluster_identifier,\n",
    "        MasterUsername=dwh_db_user,\n",
    "        MasterUserPassword=dwh_db_password,\n",
    "\n",
    "        # Parameter for role (to allow S3 access)\n",
    "        IamRoles=[role_arn],\n",
    "\n",
    "        # Make the cluster publicly accessible\n",
    "        PubliclyAccessible=True  \n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Monitor cluster status\n",
    "Format and display the cluster properties in a DataFrame. Run this block multiple times until **ClusterStatus** is **Available**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_redshift_props(props):\n",
    "    pd.set_option(\"display.max_colwidth\", None)\n",
    "    \n",
    "    keys_to_show = [\n",
    "        \"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \n",
    "        \"DBName\", \"Endpoint\", \"NumberOfNodes\", \"VpcId\", \"PubliclyAccessible\"\n",
    "    ]\n",
    "    \n",
    "    x = [(k, v) for k, v in props.items() if k in keys_to_show]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "my_cluster_props = redshift.describe_clusters(ClusterIdentifier=dwh_cluster_identifier)[\"Clusters\"][0]\n",
    "pretty_redshift_props(my_cluster_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Take note of endpoint & IAM role\n",
    "Do not run this unless **ClusterStatus** is **Available**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwh_endpoint = my_cluster_props[\"Endpoint\"][\"Address\"]\n",
    "dwh_role_arn = my_cluster_props[\"IamRoles\"][0][\"IamRoleArn\"]\n",
    "\n",
    "print(\"DWH_ENDPOINT:\", dwh_endpoint)\n",
    "print(\"DWH_ROLE_ARN:\", dwh_role_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Open network access\n",
    "## 3.1 Allow inbound traffic to Redshift by updating the security group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    vpc = ec2.Vpc(id=my_cluster_props[\"VpcId\"])\n",
    "    default_sg = list(vpc.security_groups.all())[0]\n",
    "    print(default_sg)\n",
    "\n",
    "    default_sg.authorize_ingress(\n",
    "        GroupName=default_sg.group_name,\n",
    "        CidrIp=\"0.0.0.0/0\",\n",
    "        IpProtocol=\"TCP\",\n",
    "        FromPort=int(dwh_port),\n",
    "        ToPort=int(dwh_port),\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. PostgreSQL: connect to the cluster\n",
    "\n",
    "Note: Udacity's original conn_string is: \"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB)\"\n",
    "- The string creates a successful connection in the Udacity's Workspace platform.\n",
    "- It fails to connect on my localhost.\n",
    "- Below are alternative conn_strings that work on my localhost, with preference for alt1.\n",
    "- Should alt1 fail to connect, try these alternatives:\n",
    "    - alt2:  \n",
    "conn_string = f\"postgresql://{dwh_db_user}:{dwh_db_password}@{dwh_endpoint}:{dwh_port}/{dwh_db}?sslmode=verify-full&sslrootcert=/etc/ssl/certs/amazon-trust-ca-bundle.crt\"  \n",
    "    - alt3:  \n",
    "conn_string = f\"redshift+psycopg2://{dwh_db_user}:{dwh_db_password}@{dwh_endpoint}:{dwh_port}/{dwh_db}?sslmode=verify-full&sslrootcert=system\"  \n",
    "    - alt4:  \n",
    "conn_string = f\"redshift+psycopg2://{dwh_db_user}:{dwh_db_password}@{dwh_endpoint}:{dwh_port}/{dwh_db}?sslmode=verify-full&sslrootcert=/etc/ssl/certs/amazon-trust-ca-bundle.crt\"\n",
    "    - alt5:  \n",
    "from sqlalchemy import create_engine\n",
    "conn_string = f\"redshift+psycopg2://{dwh_db_user}:{dwh_db_password}@{dwh_endpoint}:{dwh_port}/{dwh_db}?sslmode=verify-full&sslrootcert=/etc/ssl/certs/amazon-trust-ca-bundle.crt\"\n",
    "\n",
    "engine = create_engine(conn_string)\n",
    "conn = engine.connect()\n",
    "\n",
    "print(\"Connected successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_string = f\"postgresql://{dwh_db_user}:{dwh_db_password}@{dwh_endpoint}:{dwh_port}/{dwh_db}?sslmode=verify-full&sslrootcert=system\"\n",
    "%sql $conn_string\n",
    "%sql SELECT current_user;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data ingestion into Redshift\n",
    "## 4.1. Create tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "DROP TABLE IF EXISTS \"sporting_event_ticket\";\n",
    "CREATE TABLE \"sporting_event_ticket\" (\n",
    "    \"id\" double precision DEFAULT nextval('sporting_event_ticket_seq') NOT NULL,\n",
    "    \"sporting_event_id\" double precision NOT NULL,\n",
    "    \"sport_location_id\" double precision NOT NULL,\n",
    "    \"seat_level\" numeric(1,0) NOT NULL,\n",
    "    \"seat_section\" character varying(15) NOT NULL,\n",
    "    \"seat_row\" character varying(10) NOT NULL,\n",
    "    \"seat\" character varying(10) NOT NULL,\n",
    "    \"ticketholder_id\" double precision,\n",
    "    \"ticket_price\" numeric(8,2) NOT NULL\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Load Partitioned data into the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "qry = \"\"\"\n",
    "    COPY sporting_event_ticket FROM 's3://udacity-labs/tickets/split/part'\n",
    "    credentials 'aws_iam_role={}'\n",
    "    gzip delimiter ';' \n",
    "    compupdate off \n",
    "    region 'us-west-2'\n",
    "\"\"\".format(dwh_role_arn)\n",
    "\n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Create Tables for the non-partitioned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS \"sporting_event_ticket_full\";\n",
    "CREATE TABLE \"sporting_event_ticket_full\" (\n",
    "    \"id\" double precision DEFAULT nextval('sporting_event_ticket_seq') NOT NULL,\n",
    "    \"sporting_event_id\" double precision NOT NULL,\n",
    "    \"sport_location_id\" double precision NOT NULL,\n",
    "    \"seat_level\" numeric(1,0) NOT NULL,\n",
    "    \"seat_section\" character varying(15) NOT NULL,\n",
    "    \"seat_row\" character varying(10) NOT NULL,\n",
    "    \"seat\" character varying(10) NOT NULL,\n",
    "    \"ticketholder_id\" double precision,\n",
    "    \"ticket_price\" numeric(8,2) NOT NULL\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Load non-partitioned data into the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "qry = \"\"\"\n",
    "    COPY sporting_event_ticket FROM 's3://udacity-labs/tickets/full/full.csv.gz'\n",
    "    credentials 'aws_iam_role={}'\n",
    "    gzip delimiter ';' \n",
    "    compupdate off \n",
    "    region 'us-west-2'\n",
    "\"\"\".format(dwh_role_arn)\n",
    "\n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Clean up AWS resources\n",
    "Once done, delete the Redshift cluster and IAM role. DO NOT RUN THIS UNLESS YOU ARE SURE. We will be using these resources in the next exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Delete cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift.delete_cluster( ClusterIdentifier=dwh_cluster_identifier, SkipFinalClusterSnapshot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following block several times until the cluster is deleted = `ClusterNotFoundFault`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=dwh_cluster_identifier)['Clusters'][0]\n",
    "pretty_redshift_props(myClusterProps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Delete IAM role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam.detach_role_policy(RoleName=dwh_iam_role_name, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "iam.delete_role(RoleName=dwh_iam_role_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
